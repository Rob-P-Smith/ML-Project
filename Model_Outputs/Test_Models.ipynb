{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Prepare Data\n",
    "Load the CSV file using pandas, prepare features (X) and target (y), and split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv('./Outputs/pop_sea_temp_fleet_amazon_oil.csv')\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "X = data.drop(columns=['GMSL'])\n",
    "y = data['GMSL']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model\n",
    "Create and train a RandomForestRegressor, evaluate its performance using metrics like R2 score and MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Model - Mean Squared Error: 58.571252340576244\n",
      "Random Forest Model - R2 Score: 0.9859929820979106\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create and train the RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Random Forest Model - Mean Squared Error: {mse_rf}\")\n",
    "print(f\"Random Forest Model - R2 Score: {r2_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Model\n",
    "Implement linear regression using sklearn, evaluate model performance and analyze coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Model - Mean Squared Error: 75.92305171116169\n",
      "Linear Regression Model - R2 Score: 0.9818433872932102\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   GMSL   R-squared:                       0.988\n",
      "Model:                            OLS   Adj. R-squared:                  0.987\n",
      "Method:                 Least Squares   F-statistic:                     1071.\n",
      "Date:                Mon, 18 Nov 2024   Prob (F-statistic):           3.27e-96\n",
      "Time:                        23:39:30   Log-Likelihood:                -384.97\n",
      "No. Observations:                 113   AIC:                             787.9\n",
      "Df Residuals:                     104   BIC:                             812.5\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==================================================================================================\n",
      "                                     coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------------\n",
      "const                             -0.0007   7.15e-05     -9.891      0.000      -0.001      -0.001\n",
      "Unnamed: 0                         1.2928      0.129     10.005      0.000       1.037       1.549\n",
      "year                              -0.0365      0.051     -0.720      0.473      -0.137       0.064\n",
      "pop                               1.4e-08   4.11e-09      3.405      0.001    5.85e-09    2.22e-08\n",
      "popChangePercent                   0.3906      3.453      0.113      0.910      -6.457       7.239\n",
      "avgtemp                           -8.0473      7.030     -1.145      0.255     -21.988       5.893\n",
      "oilProductionInTWh                 0.0006      0.001      0.689      0.492      -0.001       0.002\n",
      "barrelsEquivalent              -1.775e-09   1.85e-09     -0.961      0.339   -5.44e-09    1.89e-09\n",
      "motorizedMerchantShipsOver100T    -0.0004      0.000     -1.550      0.124      -0.001       0.000\n",
      "annualLoss                         0.0003      0.000      1.411      0.161      -0.000       0.001\n",
      "==============================================================================\n",
      "Omnibus:                       14.096   Durbin-Watson:                   2.153\n",
      "Prob(Omnibus):                  0.001   Jarque-Bera (JB):               20.731\n",
      "Skew:                           0.605   Prob(JB):                     3.15e-05\n",
      "Kurtosis:                       4.715   Cond. No.                     8.89e+20\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.39e-20. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Create and train the LinearRegression model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Linear Regression Model - Mean Squared Error: {mse_lr}\")\n",
    "print(f\"Linear Regression Model - R2 Score: {r2_lr}\")\n",
    "\n",
    "# Analyze coefficients\n",
    "X_train_sm = sm.add_constant(X_train)  # Add constant term for intercept\n",
    "lr_model_sm = sm.OLS(y_train, X_train_sm).fit()\n",
    "\n",
    "# Print the summary of the linear regression model\n",
    "print(lr_model_sm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model\n",
    "Transform the problem for logistic regression, train the model, and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model - Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      "[[16  0]\n",
      " [ 0 13]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        29\n",
      "   macro avg       1.00      1.00      1.00        29\n",
      "weighted avg       1.00      1.00      1.00        29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Transform the target variable for logistic regression\n",
    "# Assuming GMSL is a continuous variable, we need to binarize it for logistic regression\n",
    "# Here, we will use the median value to create two classes\n",
    "median_gmsl = y.median()\n",
    "y_binary = (y > median_gmsl).astype(int)\n",
    "\n",
    "# Split the binary target variable into training and testing sets\n",
    "y_train_binary = (y_train > median_gmsl).astype(int)\n",
    "y_test_binary = (y_test > median_gmsl).astype(int)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create and train the LogisticRegression model\n",
    "log_reg_model = LogisticRegression(random_state=42)\n",
    "log_reg_model.fit(X_train_scaled, y_train_binary)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log_reg = log_reg_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_log_reg = accuracy_score(y_test_binary, y_pred_log_reg)\n",
    "conf_matrix_log_reg = confusion_matrix(y_test_binary, y_pred_log_reg)\n",
    "class_report_log_reg = classification_report(y_test_binary, y_pred_log_reg)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Logistic Regression Model - Accuracy: {accuracy_log_reg}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_log_reg)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report_log_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model Classifier\n",
    "Transform the problem for logistic regression, train the model, and evaluate its performance at classifying the GSML prediction as above or below the linear regression line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model - Accuracy: 0.8275862068965517\n",
      "\n",
      "Confusion Matrix:\n",
      "[[15  2]\n",
      " [ 3  9]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.86        17\n",
      "           1       0.82      0.75      0.78        12\n",
      "\n",
      "    accuracy                           0.83        29\n",
      "   macro avg       0.83      0.82      0.82        29\n",
      "weighted avg       0.83      0.83      0.83        29\n",
      "\n",
      "\n",
      "Future Predictions (1 = above trend, 0 = below trend):\n",
      "year 2022: Above trend\n",
      "year 2023: Above trend\n",
      "year 2024: Above trend\n",
      "year 2025: Above trend\n",
      "year 2026: Above trend\n",
      "year 2027: Above trend\n",
      "year 2028: Above trend\n",
      "year 2029: Above trend\n",
      "year 2030: Above trend\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Create trendline using years as X and GMSL as y\n",
    "years = data['year'].values.reshape(-1, 1)\n",
    "gmsl_values = data['GMSL'].values\n",
    "\n",
    "# Fit linear trend\n",
    "trend_model = LinearRegression()\n",
    "trend_model.fit(years, gmsl_values)\n",
    "\n",
    "# Calculate expected GMSL values based on trendline\n",
    "expected_gmsl = trend_model.predict(years)\n",
    "\n",
    "# Create binary target: 1 if actual GMSL is above trend, 0 if below\n",
    "y_binary = (gmsl_values > expected_gmsl).astype(int)\n",
    "\n",
    "# Split maintaining same indices as your original split\n",
    "y_train_binary = (y_train > trend_model.predict(data.loc[y_train.index, 'year'].values.reshape(-1,1))).astype(int)\n",
    "y_test_binary = (y_test > trend_model.predict(data.loc[y_test.index, 'year'].values.reshape(-1,1))).astype(int)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train logistic regression\n",
    "log_reg_model = LogisticRegression(random_state=42)\n",
    "log_reg_model.fit(X_train_scaled, y_train_binary)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log_reg = log_reg_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate model\n",
    "accuracy_log_reg = accuracy_score(y_test_binary, y_pred_log_reg)\n",
    "conf_matrix_log_reg = confusion_matrix(y_test_binary, y_pred_log_reg)\n",
    "class_report_log_reg = classification_report(y_test_binary, y_pred_log_reg)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Logistic Regression Model - Accuracy: {accuracy_log_reg}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix_log_reg)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report_log_reg)\n",
    "\n",
    "# For future predictions\n",
    "future_years = np.array(range(2022, 2031)).reshape(-1, 1)\n",
    "future_trend = trend_model.predict(future_years)\n",
    "future_X_scaled = scaler.transform(future_X)\n",
    "future_predictions_log = log_reg_model.predict(future_X_scaled)\n",
    "\n",
    "print(\"\\nFuture Predictions (1 = above trend, 0 = below trend):\")\n",
    "for year, pred in zip(future_years.flatten(), future_predictions_log):\n",
    "    print(f\"year {year}: {'Above' if pred == 1 else 'Below'} trend\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Predictions\n",
    "Generate trend-based feature values for 2022-2030 and use all models to predict future GMSL values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Model Evaluation:\n",
      "Accuracy: 0.8276\n",
      "\n",
      "Confusion Matrix:\n",
      "[[15  2]\n",
      " [ 3  9]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.86        17\n",
      "           1       0.82      0.75      0.78        12\n",
      "\n",
      "    accuracy                           0.83        29\n",
      "   macro avg       0.83      0.82      0.82        29\n",
      "weighted avg       0.83      0.83      0.83        29\n",
      "\n",
      "\n",
      "Model comparison has been saved to ./Model_Outputs/model_comparison.txt\n",
      "\n",
      "Predictions for 2022-2030:\n",
      "   year  Unnamed: 0           pop  popChangePercent    avgtemp  \\\n",
      "0  2022       142.0  7.955341e+09          0.876170  14.757305   \n",
      "1  2023       143.0  8.001387e+09          0.882340  14.764610   \n",
      "2  2024       144.0  8.047433e+09          0.888511  14.771915   \n",
      "3  2025       145.0  8.093479e+09          0.894681  14.779220   \n",
      "4  2026       146.0  8.139525e+09          0.900851  14.786525   \n",
      "5  2027       147.0  8.185571e+09          0.907021  14.793830   \n",
      "6  2028       148.0  8.231617e+09          0.913191  14.801135   \n",
      "7  2029       149.0  8.277663e+09          0.919362  14.808440   \n",
      "8  2030       150.0  8.323709e+09          0.925532  14.815745   \n",
      "\n",
      "   oilProductionInTWh  barrelsEquivalent  motorizedMerchantShipsOver100T  \\\n",
      "0        48347.843733       1.783354e+10                    98317.375887   \n",
      "1        48688.321506       1.795913e+10                    99009.751773   \n",
      "2        49028.799278       1.808472e+10                    99702.127660   \n",
      "3        49369.277051       1.821031e+10                   100394.503546   \n",
      "4        49709.754824       1.833590e+10                   101086.879433   \n",
      "5        50050.232597       1.846148e+10                   101779.255319   \n",
      "6        50390.710369       1.858707e+10                   102471.631206   \n",
      "7        50731.188142       1.871266e+10                   103164.007092   \n",
      "8        51071.665915       1.883825e+10                   103856.382979   \n",
      "\n",
      "     annualLoss  GMSL_Prediction_RF  GMSL_Prediction_LR  Above_Trend  \n",
      "0  10550.297872            51.60382           65.868844            1  \n",
      "1  10624.595745            51.67675           67.454033            1  \n",
      "2  10698.893617            51.67675           69.039222            1  \n",
      "3  10773.191489            51.62254           70.624411            1  \n",
      "4  10847.489362            51.40509           72.209601            1  \n",
      "5  10921.787234            51.85503           73.794790            1  \n",
      "6  10996.085106            51.85503           75.379979            1  \n",
      "7  11070.382979            51.86355           76.965168            1  \n",
      "8  11144.680851            51.69558           78.550358            1  \n"
     ]
    }
   ],
   "source": [
    "# Add these imports at the top\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Add this code after the existing linear regression but before future predictions\n",
    "\n",
    "# Create trendline for classification boundary\n",
    "years = data['year'].values.reshape(-1, 1)\n",
    "gmsl_values = data['GMSL'].values\n",
    "trend_model = LinearRegression()\n",
    "trend_model.fit(years, gmsl_values)\n",
    "trend_line = trend_model.predict(years)\n",
    "\n",
    "# Create binary target (1 if above trend, 0 if below)\n",
    "y_binary = (gmsl_values > trend_line).astype(int)\n",
    "\n",
    "# Split binary target\n",
    "y_train_binary = (y_train > trend_model.predict(data.loc[y_train.index, 'year'].values.reshape(-1,1))).astype(int)\n",
    "y_test_binary = (y_test > trend_model.predict(data.loc[y_test.index, 'year'].values.reshape(-1,1))).astype(int)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train logistic regression\n",
    "log_model = LogisticRegression(random_state=42)\n",
    "log_model.fit(X_train_scaled, y_train_binary)\n",
    "\n",
    "# Predict future trends\n",
    "future_years_array = future_years['year'].values.reshape(-1, 1)\n",
    "future_trend = trend_model.predict(future_years_array)\n",
    "future_X_scaled = scaler.transform(future_X)\n",
    "future_predictions_log = log_model.predict(future_X_scaled)\n",
    "\n",
    "# Add to results\n",
    "future_years['Above_Trend'] = future_predictions_log\n",
    "\n",
    "# Add these evaluation metrics before final print\n",
    "print(\"\\nLogistic Regression Model Evaluation:\")\n",
    "y_pred_log = log_model.predict(X_test_scaled)\n",
    "print(f\"Accuracy: {accuracy_score(y_test_binary, y_pred_log):.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test_binary, y_pred_log))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_binary, y_pred_log))\n",
    "\n",
    "# Add after existing code but before final prints\n",
    "import os\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Create metrics for each model\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "log_pred_proba = log_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "metrics = pd.DataFrame({\n",
    "    'Random Forest': {\n",
    "        'R2 Score': r2_score(y_test, rf_pred),\n",
    "        'MSE': mean_squared_error(y_test, rf_pred),\n",
    "        'MAE': mean_absolute_error(y_test, rf_pred),\n",
    "        'Feature Importance': dict(zip(X.columns, rf_model.feature_importances_))\n",
    "    },\n",
    "    'Linear Regression': {\n",
    "        'R2 Score': r2_score(y_test, lr_pred),\n",
    "        'MSE': mean_squared_error(y_test, lr_pred),\n",
    "        'MAE': mean_absolute_error(y_test, lr_pred),\n",
    "        'Coefficients': dict(zip(X.columns, lr_model.coef_))\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'Accuracy': accuracy_score(y_test_binary, y_pred_log),\n",
    "        'Feature Coefficients': dict(zip(X.columns, log_model.coef_[0])),\n",
    "        'Classification Report': classification_report(y_test_binary, y_pred_log)\n",
    "    }\n",
    "})\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('./Model_Outputs', exist_ok=True)\n",
    "\n",
    "# Write comparison to file\n",
    "with open('./Model_Outputs/model_comparison.txt', 'w') as f:\n",
    "    f.write(\"Model Comparison Results\\n\")\n",
    "    f.write(\"=======================\\n\\n\")\n",
    "    \n",
    "    for model in metrics.columns:\n",
    "        f.write(f\"{model}\\n\")\n",
    "        f.write(\"-\" * len(model) + \"\\n\")\n",
    "        for metric, value in metrics[model].items():\n",
    "            f.write(f\"{metric}:\\n\")\n",
    "            if isinstance(value, dict):\n",
    "                for k, v in value.items():\n",
    "                    f.write(f\"  {k}: {v:.4f}\\n\")\n",
    "            elif isinstance(value, str):\n",
    "                f.write(f\"{value}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{value:.4f}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"\\nModel comparison has been saved to ./Model_Outputs/model_comparison.txt\")\n",
    "\n",
    "# Modify final print to include logistic results\n",
    "print(\"\\nPredictions for 2022-2030:\")\n",
    "print(future_years)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "# # Load data\n",
    "# data = pd.read_csv('./Outputs/pop_sea_temp_fleet_amazon_oil.csv')\n",
    "\n",
    "# # Separate features and target\n",
    "# X = data.drop(['GMSL', 'year'], axis=1)\n",
    "# y = data['GMSL']\n",
    "\n",
    "# # Split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Train models\n",
    "# rf_model = RandomForestRegressor(random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# lr_model = LinearRegression()\n",
    "# lr_model.fit(X_train, y_train)\n",
    "\n",
    "# # Create future years dataframe\n",
    "# future_years = pd.DataFrame()\n",
    "# future_years['year'] = range(2022, 2031)\n",
    "\n",
    "# # For each feature, extrapolate based on trend\n",
    "# for column in X.columns:\n",
    "#     yearly_change = data[column].diff().mean()\n",
    "#     last_value = data[column].iloc[-1]\n",
    "#     future_values = [last_value + (i * yearly_change) for i in range(1, 10)]\n",
    "#     future_years[column] = future_values\n",
    "\n",
    "# # Make predictions\n",
    "# future_X = future_years[X.columns]\n",
    "# future_predictions_rf = rf_model.predict(future_X)\n",
    "# future_predictions_lr = lr_model.predict(future_X)\n",
    "\n",
    "# # Add predictions to results\n",
    "# future_years['GMSL_Prediction_RF'] = future_predictions_rf\n",
    "# future_years['GMSL_Prediction_LR'] = future_predictions_lr\n",
    "\n",
    "# # Print model statistics\n",
    "# print(\"\\nRandom Forest Feature Importance:\")\n",
    "# for feature, importance in zip(X.columns, rf_model.feature_importances_):\n",
    "#     print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# # For Linear Regression statistics using statsmodels\n",
    "# X_with_constant = sm.add_constant(X_train)\n",
    "# model = sm.OLS(y_train, X_with_constant).fit()\n",
    "# print(\"\\nLinear Regression Statistics:\")\n",
    "# print(model.summary())\n",
    "\n",
    "# # Display predictions\n",
    "# print(\"\\nPredictions for 2022-2030:\")\n",
    "# print(future_years)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
